{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0b093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import interpolate\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VQ-VAE training.\n",
    "    Only needs high-resolution images for reconstruction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hr_arrays):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hr_arrays: torch.Tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        assert isinstance(hr_arrays, torch.Tensor), \"hr_arrays must be a torch.Tensor\"\n",
    "        assert hr_arrays.ndim == 4, f\"hr_arrays must be 4D tensor, got shape {hr_arrays.shape}\"\n",
    "        \n",
    "        self.hr = hr_arrays\n",
    "        self.num_samples = hr_arrays.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dictionary with 'image' key.\n",
    "        VQModelInterface expects shape (H, W, C) for input.\n",
    "        \"\"\"\n",
    "        hr_sample = self.hr[idx]  # Shape: (C, H, W)\n",
    "        hr_sample = hr_sample.permute(1, 2, 0)  # Convert to (H, W, C)\n",
    "        \n",
    "        return {'image': hr_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 460\n",
      "Val samples: 52\n",
      "HR tensor shape: torch.Size([512, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load JAX-CFD normalized data (range [-1,1])\n",
    "path = 'data/data_normalized.h5'\n",
    "hr_data = torch.from_numpy(h5py.File(path, 'r')['hr'][:])\n",
    "\n",
    "# Create RGB tensor from data\n",
    "hr_tensor = torch.stack([hr_data for _ in range(3)], 1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(hr_tensor))\n",
    "val_size = len(hr_tensor) - train_size\n",
    "\n",
    "hr_train = hr_tensor[:train_size]\n",
    "hr_val = hr_tensor[train_size:]\n",
    "\n",
    "train_dataset = VQVAEDataset(hr_train)\n",
    "val_dataset = VQVAEDataset(hr_val)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"HR tensor shape: {hr_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create_dataloaders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 230\n",
      "Val batches: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set PyTorch DataLoaders\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "manual_vqvae_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualVQVAE(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Wrapper for VQModelInterface that uses manual optimization.\n",
    "    Required for PyTorch Lightning >= 2.0 which doesn't support optimizer_idx.\n",
    "    \"\"\"\n",
    "    def __init__(self, vqvae_model):\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae_model\n",
    "        # Enable manual optimization\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vqvae(x)\n",
    "    \n",
    "    def get_input(self, batch, k):\n",
    "        return self.vqvae.get_input(batch, k)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_ae, opt_disc = self.optimizers()\n",
    "        \n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        # Optimize autoencoder\n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        opt_ae.zero_grad()\n",
    "        self.manual_backward(aeloss)\n",
    "        opt_ae.step()\n",
    "        \n",
    "        # Optimize discriminator\n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\"\n",
    "        )\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        self.manual_backward(discloss)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "#         self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"], prog_bar=True, sync_dist=True)\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.vqvae.learning_rate\n",
    "        \n",
    "        opt_ae = torch.optim.Adam(\n",
    "            list(self.vqvae.encoder.parameters()) +\n",
    "            list(self.vqvae.decoder.parameters()) +\n",
    "            list(self.vqvae.quantize.parameters()) +\n",
    "            list(self.vqvae.quant_conv.parameters()) +\n",
    "            list(self.vqvae.post_quant_conv.parameters()),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        opt_disc = torch.optim.Adam(\n",
    "            self.vqvae.loss.discriminator.parameters(),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        return [opt_ae, opt_disc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "create_vqvae_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQ-VAE config created\n",
      "target: ldm.autoencoder.VQModelInterface\n",
      "params:\n",
      "  embed_dim: 3\n",
      "  n_embed: 8192\n",
      "  ddconfig:\n",
      "    double_z: false\n",
      "    z_channels: 3\n",
      "    resolution: 256\n",
      "    in_channels: 3\n",
      "    out_ch: 3\n",
      "    ch: 64\n",
      "    ch_mult:\n",
      "    - 1\n",
      "    - 2\n",
      "    - 4\n",
      "    num_res_blocks: 2\n",
      "    attn_resolutions: []\n",
      "    dropout: 0.0\n",
      "  lossconfig:\n",
      "    target: ldm.vqperceptual.VQLPIPSWithDiscriminator\n",
      "    params:\n",
      "      disc_conditional: false\n",
      "      disc_in_channels: 3\n",
      "      disc_start: 10000\n",
      "      disc_weight: 0.8\n",
      "      codebook_weight: 1.0\n",
      "  image_key: image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create VQ-VAE config\n",
    "# Based on the VQModelInterface structure from autoencoder.py\n",
    "vqvae_config = OmegaConf.create({\n",
    "    'target': 'ldm.autoencoder.VQModelInterface',\n",
    "    'params': {\n",
    "        'embed_dim': 3,\n",
    "        'n_embed': 8192,\n",
    "        'ddconfig': {\n",
    "            'double_z': False,\n",
    "            'z_channels': 3,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 64,\n",
    "            'ch_mult': [1, 2, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'lossconfig': {\n",
    "            'target': 'ldm.vqperceptual.VQLPIPSWithDiscriminator',\n",
    "            'params': {\n",
    "                'disc_conditional': False,\n",
    "                'disc_in_channels': 3,\n",
    "                'disc_start': 10000,\n",
    "                'disc_weight': 0.8,\n",
    "                'codebook_weight': 1.0\n",
    "            }\n",
    "        },\n",
    "        'image_key': 'image',\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"VQ-VAE config created\")\n",
    "print(OmegaConf.to_yaml(vqvae_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "VQLPIPSWithDiscriminator: Running with LPIPS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Training VQ-VAE from scratch\n",
      "\n",
      "VQ-VAE model type: <class 'ldm.autoencoder.VQModelInterface'>\n",
      "Parameters: 31.35M\n",
      "Trainable parameters: 16.64M\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VQModelInterface from config\n",
    "vqvae_model = instantiate_from_config(vqvae_config)\n",
    "\n",
    "# Set learning rate (VQModel expects this attribute)\n",
    "vqvae_model.learning_rate = 4.5e-6\n",
    "\n",
    "# Optional: Load pretrained weights\n",
    "load_pretrained = False\n",
    "if load_pretrained:\n",
    "    ckpt_path = \"model.ckpt\"\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    \n",
    "    # Extract only VQ-VAE weights (first_stage_model)\n",
    "    vqvae_sd = {k.replace('first_stage_model.', ''): v \n",
    "                for k, v in sd.items() \n",
    "                if k.startswith('first_stage_model.')}\n",
    "    \n",
    "    vqvae_model.load_state_dict(vqvae_sd, strict=False)\n",
    "    print(\"Loaded pretrained VQ-VAE weights\")\n",
    "else:\n",
    "    print(\"Training VQ-VAE from scratch\")\n",
    "\n",
    "print(f\"\\nVQ-VAE model type: {type(vqvae_model)}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vqvae_model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in vqvae_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374bed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped VQ-VAE in ManualVQVAE for PyTorch Lightning compatibility\n"
     ]
    }
   ],
   "source": [
    "# Wrap VQ-VAE model with manual optimization wrapper\n",
    "lightning_model = ManualVQVAE(vqvae_model)\n",
    "print(\"Wrapped VQ-VAE in ManualVQVAE for PyTorch Lightning compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured\n",
      "Max epochs: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/vqvae',\n",
    "    filename='vqvae-{epoch:02d}-{val/rec_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val/rec_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Setup trainer\n",
    "# Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    logger=True,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Max epochs: {trainer.max_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VQ-VAE training...\n",
      "Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------\n",
      "0 | vqvae | VQModelInterface | 31.4 M | train | 0    \n",
      "-----------------------------------------------------------\n",
      "16.6 M    Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "31.4 M    Total params\n",
      "125.408   Total estimated model params size (MB)\n",
      "188       Modules in train mode\n",
      "58        Modules in eval mode\n",
      "0         Total Flops\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting VQ-VAE training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: VQModel uses 2 optimizers (autoencoder + discriminator)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    623\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    624\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    626\u001b[0m     ckpt_path,\n\u001b[1;32m    627\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m )\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1079\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1121\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1121\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1150\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:146\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:441\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    435\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    440\u001b[0m )\n\u001b[0;32m--> 441\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mManualVQVAE.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mimage_key)\n\u001b[0;32m---> 53\u001b[0m     xrec, qloss, ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_pred_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     aeloss, log_dict_ae \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mloss(\n\u001b[1;32m     56\u001b[0m         qloss, x, xrec, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step,\n\u001b[1;32m     57\u001b[0m         last_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mget_last_layer(),\n\u001b[1;32m     58\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m         predicted_indices\u001b[38;5;241m=\u001b[39mind\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m     discloss, log_dict_disc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mloss(\n\u001b[1;32m     63\u001b[0m         qloss, x, xrec, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step,\n\u001b[1;32m     64\u001b[0m         last_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mget_last_layer(),\n\u001b[1;32m     65\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     66\u001b[0m         predicted_indices\u001b[38;5;241m=\u001b[39mind\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/coop/drozda/diffusion-kolmogorov-flow/ldm/autoencoder.py:123\u001b[0m, in \u001b[0;36mVQModel.forward\u001b[0;34m(self, input, return_pred_indices)\u001b[0m\n\u001b[1;32m    121\u001b[0m encode_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Get the first 3 values\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m quant, diff, info \u001b[38;5;241m=\u001b[39m encode_output[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Extract index from info if it has the expected structure\u001b[39;00m\n\u001b[1;32m    125\u001b[0m ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Train VQ-VAE using ManualVQVAE wrapper\n",
    "print(\"Starting VQ-VAE training...\")\n",
    "print(\"Note: VQModel uses 2 optimizers (autoencoder + discriminator)\")\n",
    "trainer.fit(lightning_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final VQ-VAE model\n",
    "vqvae_path = \"vqvae_trained.ckpt\"\n",
    "torch.save({\n",
    "    'state_dict': lightning_model.vqvae.state_dict(),\n",
    "    'config': vqvae_config,\n",
    "}, vqvae_path)\n",
    "\n",
    "print(f\"VQ-VAE saved to {vqvae_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_reconstructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "vqvae_model = lightning_model.vqvae\n",
    "vqvae_model.eval()\n",
    "device = next(vqvae_model.parameters()).device\n",
    "\n",
    "# Get a batch from validation set\n",
    "batch = next(iter(val_loader))\n",
    "hr_images = vqvae_model.get_input(batch, 'image').to(device)[:8]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, _ = vqvae_model(hr_images)\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(hr_images[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vqvae_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding and decoding separately\n",
    "print(\"Testing VQ-VAE encode/decode pipeline...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get one sample\n",
    "    test_img = hr_images[:1]\n",
    "    print(f\"Input shape: {test_img.shape}\")\n",
    "    \n",
    "    # Encode (returns continuous latent)\n",
    "    latent = vqvae_model.encode(test_img)\n",
    "    print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    \n",
    "    # Decode (includes quantization)\n",
    "    reconstructed = vqvae_model.decode(latent)\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    rec_error = torch.nn.functional.l1_loss(reconstructed, test_img)\n",
    "    print(f\"Reconstruction L1 error: {rec_error.item():.6f}\")\n",
    "\n",
    "print(\"\\nVQ-VAE is ready for use in latent diffusion!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-kolmogorov-flow-env",
   "language": "python",
   "name": "diffusion-kolmogorov-flow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
