{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0b093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import interpolate\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VQ-VAE training.\n",
    "    Only needs high-resolution images for reconstruction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hr_arrays):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hr_arrays: torch.Tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        assert isinstance(hr_arrays, torch.Tensor), \"hr_arrays must be a torch.Tensor\"\n",
    "        assert hr_arrays.ndim == 4, f\"hr_arrays must be 4D tensor, got shape {hr_arrays.shape}\"\n",
    "        \n",
    "        self.hr = hr_arrays\n",
    "        self.num_samples = hr_arrays.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dictionary with 'image' key.\n",
    "        VQModelInterface expects shape (H, W, C) for input.\n",
    "        \"\"\"\n",
    "        hr_sample = self.hr[idx]  # Shape: (C, H, W)\n",
    "        hr_sample = hr_sample.permute(1, 2, 0)  # Convert to (H, W, C)\n",
    "        \n",
    "        return {'image': hr_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 460\n",
      "Val samples: 52\n",
      "HR tensor shape: torch.Size([512, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load JAX-CFD normalized data (range [-1,1])\n",
    "path = 'data/data_normalized.h5'\n",
    "hr_data = torch.from_numpy(h5py.File(path, 'r')['hr'][:])\n",
    "\n",
    "# Create RGB tensor from data\n",
    "hr_tensor = torch.stack([hr_data for _ in range(3)], 1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(hr_tensor))\n",
    "val_size = len(hr_tensor) - train_size\n",
    "\n",
    "hr_train = hr_tensor[:train_size]\n",
    "hr_val = hr_tensor[train_size:]\n",
    "\n",
    "train_dataset = VQVAEDataset(hr_train)\n",
    "val_dataset = VQVAEDataset(hr_val)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"HR tensor shape: {hr_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create_dataloaders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 15\n",
      "Val batches: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set PyTorch DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create_vqvae_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQ-VAE config created\n",
      "target: ldm.autoencoder.VQModelInterface\n",
      "params:\n",
      "  embed_dim: 3\n",
      "  n_embed: 8192\n",
      "  ddconfig:\n",
      "    double_z: false\n",
      "    z_channels: 3\n",
      "    resolution: 256\n",
      "    in_channels: 3\n",
      "    out_ch: 3\n",
      "    ch: 128\n",
      "    ch_mult:\n",
      "    - 1\n",
      "    - 2\n",
      "    - 4\n",
      "    num_res_blocks: 2\n",
      "    attn_resolutions: []\n",
      "    dropout: 0.0\n",
      "  lossconfig:\n",
      "    target: ldm.vqperceptual.VQLPIPSWithDiscriminator\n",
      "    params:\n",
      "      disc_conditional: false\n",
      "      disc_in_channels: 3\n",
      "      disc_start: 10000\n",
      "      disc_weight: 0.8\n",
      "      codebook_weight: 1.0\n",
      "  image_key: image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create VQ-VAE config\n",
    "# Based on the VQModelInterface structure from autoencoder.py\n",
    "vqvae_config = OmegaConf.create({\n",
    "    'target': 'ldm.autoencoder.VQModelInterface',\n",
    "    'params': {\n",
    "        'embed_dim': 3,\n",
    "        'n_embed': 8192,\n",
    "        'ddconfig': {\n",
    "            'double_z': False,\n",
    "            'z_channels': 3,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 128,\n",
    "            'ch_mult': [1, 2, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'lossconfig': {\n",
    "            'target': 'ldm.vqperceptual.VQLPIPSWithDiscriminator',\n",
    "            'params': {\n",
    "                'disc_conditional': False,\n",
    "                'disc_in_channels': 3,\n",
    "                'disc_start': 10000,\n",
    "                'disc_weight': 0.8,\n",
    "                'codebook_weight': 1.0\n",
    "            }\n",
    "        },\n",
    "        'image_key': 'image',\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"VQ-VAE config created\")\n",
    "print(OmegaConf.to_yaml(vqvae_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "VQLPIPSWithDiscriminator: Running with LPIPS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Training VQ-VAE from scratch\n",
      "\n",
      "VQ-VAE model type: <class 'ldm.autoencoder.VQModelInterface'>\n",
      "Parameters: 72.80M\n",
      "Trainable parameters: 58.09M\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VQModelInterface from config\n",
    "vqvae_model = instantiate_from_config(vqvae_config)\n",
    "\n",
    "# Set learning rate (VQModel expects this attribute)\n",
    "vqvae_model.learning_rate = 4.5e-6\n",
    "\n",
    "# Optional: Load pretrained weights\n",
    "load_pretrained = False\n",
    "if load_pretrained:\n",
    "    ckpt_path = \"model.ckpt\"\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    \n",
    "    # Extract only VQ-VAE weights (first_stage_model)\n",
    "    vqvae_sd = {k.replace('first_stage_model.', ''): v \n",
    "                for k, v in sd.items() \n",
    "                if k.startswith('first_stage_model.')}\n",
    "    \n",
    "    vqvae_model.load_state_dict(vqvae_sd, strict=False)\n",
    "    print(\"Loaded pretrained VQ-VAE weights\")\n",
    "else:\n",
    "    print(\"Training VQ-VAE from scratch\")\n",
    "\n",
    "print(f\"\\nVQ-VAE model type: {type(vqvae_model)}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vqvae_model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in vqvae_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured\n",
      "Max epochs: 100\n",
      "Gradient clip: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/vqvae',\n",
    "    filename='vqvae-{epoch:02d}-{val/rec_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val/rec_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Setup trainer\n",
    "# Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    logger=True,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Max epochs: {trainer.max_epochs}\")\n",
    "print(f\"Gradient clip: {trainer.gradient_clip_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VQ-VAE training...\n",
      "Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_d 4.5e-06\n",
      "lr_g 4.5e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx` argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers in `training_step` with `opt1, opt2, ... = self.optimizers()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting VQ-VAE training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: VQModel uses 2 optimizers (autoencoder + discriminator)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvqvae_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m    623\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    624\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    626\u001b[0m     ckpt_path,\n\u001b[1;32m    627\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m )\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:159\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:139\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:194\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    188\u001b[0m optimizers, lr_schedulers, monitor \u001b[38;5;241m=\u001b[39m _configure_optimizers(optim_conf)\n\u001b[1;32m    189\u001b[0m lr_scheduler_configs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     _configure_schedulers_automatic_opt(lr_schedulers, monitor)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _configure_schedulers_manual_opt(lr_schedulers)\n\u001b[1;32m    193\u001b[0m )\n\u001b[0;32m--> 194\u001b[0m \u001b[43m_validate_multiple_optimizers_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m _validate_optimizers_attached(optimizers, lr_scheduler_configs)\n\u001b[1;32m    196\u001b[0m _validate_scheduler_api(lr_scheduler_configs, model)\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:353\u001b[0m, in \u001b[0;36m_validate_multiple_optimizers_support\u001b[0;34m(optimizers, model)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_multiple_optimizers_support\u001b[39m(optimizers: \u001b[38;5;28mlist\u001b[39m[Optimizer], model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_param_in_hook_signature(model\u001b[38;5;241m.\u001b[39mtraining_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `training_step` with `opt1, opt2, ... = self.optimizers()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         )\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with multiple optimizers is only supported with manual optimization. Set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `self.automatic_optimization = False`, then access your optimizers in `training_step` with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `opt1, opt2, ... = self.optimizers()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx` argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers in `training_step` with `opt1, opt2, ... = self.optimizers()`."
     ]
    }
   ],
   "source": [
    "# Train VQ-VAE using VQModelInterface\n",
    "print(\"Starting VQ-VAE training...\")\n",
    "print(\"Note: VQModel uses 2 optimizers (autoencoder + discriminator)\")\n",
    "trainer.fit(vqvae_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final VQ-VAE model\n",
    "vqvae_path = \"vqvae_trained.ckpt\"\n",
    "torch.save({\n",
    "    'state_dict': vqvae_model.state_dict(),\n",
    "    'config': vqvae_config,\n",
    "}, vqvae_path)\n",
    "\n",
    "print(f\"VQ-VAE saved to {vqvae_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_reconstructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "vqvae_model.eval()\n",
    "device = next(vqvae_model.parameters()).device\n",
    "\n",
    "# Get a batch from validation set\n",
    "batch = next(iter(val_loader))\n",
    "hr_images = vqvae_model.get_input(batch, 'image').to(device)[:8]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, _ = vqvae_model(hr_images)\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(hr_images[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vqvae_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding and decoding separately\n",
    "print(\"Testing VQ-VAE encode/decode pipeline...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get one sample\n",
    "    test_img = hr_images[:1]\n",
    "    print(f\"Input shape: {test_img.shape}\")\n",
    "    \n",
    "    # Encode (returns continuous latent)\n",
    "    latent = vqvae_model.encode(test_img)\n",
    "    print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    \n",
    "    # Decode (includes quantization)\n",
    "    reconstructed = vqvae_model.decode(latent)\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    rec_error = torch.nn.functional.l1_loss(reconstructed, test_img)\n",
    "    print(f\"Reconstruction L1 error: {rec_error.item():.6f}\")\n",
    "\n",
    "print(\"\\nVQ-VAE is ready for use in latent diffusion!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-kolmogorov-flow-env",
   "language": "python",
   "name": "diffusion-kolmogorov-flow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
