{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0b093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import interpolate\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VQ-VAE training.\n",
    "    Only needs high-resolution images for reconstruction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hr_arrays):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hr_arrays: torch.Tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        assert isinstance(hr_arrays, torch.Tensor), \"hr_arrays must be a torch.Tensor\"\n",
    "        assert hr_arrays.ndim == 4, f\"hr_arrays must be 4D tensor, got shape {hr_arrays.shape}\"\n",
    "        \n",
    "        self.hr = hr_arrays\n",
    "        self.num_samples = hr_arrays.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dictionary with 'image' key.\n",
    "        VQModelInterface expects shape (H, W, C) for input.\n",
    "        \"\"\"\n",
    "        hr_sample = self.hr[idx]  # Shape: (C, H, W)\n",
    "        hr_sample = hr_sample.permute(1, 2, 0)  # Convert to (H, W, C)\n",
    "        \n",
    "        return {'image': hr_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 460\n",
      "Val samples: 52\n",
      "HR tensor shape: torch.Size([512, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load JAX-CFD normalized data (range [-1,1])\n",
    "path = 'data/data_normalized.h5'\n",
    "hr_data = torch.from_numpy(h5py.File(path, 'r')['hr'][:])\n",
    "\n",
    "# Create RGB tensor from data\n",
    "hr_tensor = torch.stack([hr_data for _ in range(3)], 1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(hr_tensor))\n",
    "val_size = len(hr_tensor) - train_size\n",
    "\n",
    "hr_train = hr_tensor[:train_size]\n",
    "hr_val = hr_tensor[train_size:]\n",
    "\n",
    "train_dataset = VQVAEDataset(hr_train)\n",
    "val_dataset = VQVAEDataset(hr_val)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"HR tensor shape: {hr_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "create_dataloaders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 15\n",
      "Val batches: 2\n"
     ]
    }
   ],
   "source": [
    "# Set PyTorch DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual_vqvae_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualVQVAE(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Wrapper for VQModelInterface that uses manual optimization.\n",
    "    Required for PyTorch Lightning >= 2.0 which doesn't support optimizer_idx.\n",
    "    \"\"\"\n",
    "    def __init__(self, vqvae_model):\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae_model\n",
    "        # Enable manual optimization\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vqvae(x)\n",
    "    \n",
    "    def get_input(self, batch, k):\n",
    "        return self.vqvae.get_input(batch, k)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_ae, opt_disc = self.optimizers()\n",
    "        \n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        # Optimize autoencoder\n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        opt_ae.zero_grad()\n",
    "        self.manual_backward(aeloss)\n",
    "        opt_ae.step()\n",
    "        \n",
    "        # Optimize discriminator\n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\"\n",
    "        )\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        self.manual_backward(discloss)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "#         self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"], prog_bar=True, sync_dist=True)\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.vqvae.learning_rate\n",
    "        \n",
    "        opt_ae = torch.optim.Adam(\n",
    "            list(self.vqvae.encoder.parameters()) +\n",
    "            list(self.vqvae.decoder.parameters()) +\n",
    "            list(self.vqvae.quantize.parameters()) +\n",
    "            list(self.vqvae.quant_conv.parameters()) +\n",
    "            list(self.vqvae.post_quant_conv.parameters()),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        opt_disc = torch.optim.Adam(\n",
    "            self.vqvae.loss.discriminator.parameters(),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        return [opt_ae, opt_disc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "create_vqvae_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQ-VAE config created\n",
      "target: ldm.autoencoder.VQModelInterface\n",
      "params:\n",
      "  embed_dim: 3\n",
      "  n_embed: 8192\n",
      "  ddconfig:\n",
      "    double_z: false\n",
      "    z_channels: 3\n",
      "    resolution: 256\n",
      "    in_channels: 3\n",
      "    out_ch: 3\n",
      "    ch: 64\n",
      "    ch_mult:\n",
      "    - 1\n",
      "    - 2\n",
      "    - 4\n",
      "    num_res_blocks: 2\n",
      "    attn_resolutions: []\n",
      "    dropout: 0.0\n",
      "  lossconfig:\n",
      "    target: ldm.vqperceptual.VQLPIPSWithDiscriminator\n",
      "    params:\n",
      "      disc_conditional: false\n",
      "      disc_in_channels: 3\n",
      "      disc_start: 10000\n",
      "      disc_weight: 0.8\n",
      "      codebook_weight: 1.0\n",
      "  image_key: image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create VQ-VAE config\n",
    "# Based on the VQModelInterface structure from autoencoder.py\n",
    "vqvae_config = OmegaConf.create({\n",
    "    'target': 'ldm.autoencoder.VQModelInterface',\n",
    "    'params': {\n",
    "        'embed_dim': 3,\n",
    "        'n_embed': 8192,\n",
    "        'ddconfig': {\n",
    "            'double_z': False,\n",
    "            'z_channels': 3,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 64,\n",
    "            'ch_mult': [1, 2, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'lossconfig': {\n",
    "            'target': 'ldm.vqperceptual.VQLPIPSWithDiscriminator',\n",
    "            'params': {\n",
    "                'disc_conditional': False,\n",
    "                'disc_in_channels': 3,\n",
    "                'disc_start': 10000,\n",
    "                'disc_weight': 0.8,\n",
    "                'codebook_weight': 1.0\n",
    "            }\n",
    "        },\n",
    "        'image_key': 'image',\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"VQ-VAE config created\")\n",
    "print(OmegaConf.to_yaml(vqvae_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 256 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "VQLPIPSWithDiscriminator: Running with LPIPS.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Training VQ-VAE from scratch\n",
      "\n",
      "VQ-VAE model type: <class 'ldm.autoencoder.VQModelInterface'>\n",
      "Parameters: 31.35M\n",
      "Trainable parameters: 16.64M\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VQModelInterface from config\n",
    "vqvae_model = instantiate_from_config(vqvae_config)\n",
    "\n",
    "# Set learning rate (VQModel expects this attribute)\n",
    "vqvae_model.learning_rate = 4.5e-6\n",
    "\n",
    "# Optional: Load pretrained weights\n",
    "load_pretrained = False\n",
    "if load_pretrained:\n",
    "    ckpt_path = \"model.ckpt\"\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    \n",
    "    # Extract only VQ-VAE weights (first_stage_model)\n",
    "    vqvae_sd = {k.replace('first_stage_model.', ''): v \n",
    "                for k, v in sd.items() \n",
    "                if k.startswith('first_stage_model.')}\n",
    "    \n",
    "    vqvae_model.load_state_dict(vqvae_sd, strict=False)\n",
    "    print(\"Loaded pretrained VQ-VAE weights\")\n",
    "else:\n",
    "    print(\"Training VQ-VAE from scratch\")\n",
    "\n",
    "print(f\"\\nVQ-VAE model type: {type(vqvae_model)}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vqvae_model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in vqvae_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "374bed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped VQ-VAE in ManualVQVAE for PyTorch Lightning compatibility\n"
     ]
    }
   ],
   "source": [
    "# Wrap VQ-VAE model with manual optimization wrapper\n",
    "lightning_model = ManualVQVAE(vqvae_model)\n",
    "print(\"Wrapped VQ-VAE in ManualVQVAE for PyTorch Lightning compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "setup_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured\n",
      "Max epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/vqvae',\n",
    "    filename='vqvae-{epoch:02d}-{val/rec_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val/rec_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Setup trainer\n",
    "# Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    logger=True,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Max epochs: {trainer.max_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------\n",
      "0 | vqvae | VQModelInterface | 31.4 M | train | 0    \n",
      "-----------------------------------------------------------\n",
      "16.6 M    Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "31.4 M    Total params\n",
      "125.408   Total estimated model params size (MB)\n",
      "188       Modules in train mode\n",
      "58        Modules in eval mode\n",
      "0         Total Flops\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VQ-VAE training...\n",
      "Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n",
      "Epoch 0: 100%|██████████| 15/15 [00:06<00:00,  2.20it/s, v_num=44]         \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.55it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.60it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 15/15 [00:06<00:00,  2.19it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.58it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.79it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.61it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.66it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.71it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.59it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 15/15 [00:06<00:00,  2.17it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 14.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.60it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 15/15 [00:06<00:00,  2.17it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.50it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.19it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.54it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 15/15 [00:06<00:00,  2.18it/s, v_num=44]    \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 15/15 [00:07<00:00,  2.01it/s, v_num=44]    \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 15/15 [00:08<00:00,  1.74it/s, v_num=44]\n"
     ]
    }
   ],
   "source": [
    "# Train VQ-VAE using ManualVQVAE wrapper\n",
    "print(\"Starting VQ-VAE training...\")\n",
    "print(\"Note: VQModel uses 2 optimizers (autoencoder + discriminator)\")\n",
    "trainer.fit(lightning_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "save_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQ-VAE saved to vqvae_trained.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save final VQ-VAE model\n",
    "vqvae_path = \"vqvae_trained.ckpt\"\n",
    "torch.save({\n",
    "    'state_dict': lightning_model.vqvae.state_dict(),\n",
    "    'config': vqvae_config,\n",
    "}, vqvae_path)\n",
    "\n",
    "print(f\"VQ-VAE saved to {vqvae_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "visualize_reconstructions",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected encode output length: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m         reconstructed[i], _ \u001b[38;5;241m=\u001b[39m \u001b[43mvqvae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhr_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Plot original vs reconstructed\u001b[39;00m\n\u001b[1;32m     17\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m8\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/coop/drozda/diffusion-kolmogorov-flow/ldm/autoencoder.py:131\u001b[0m, in \u001b[0;36mVQModel.forward\u001b[0;34m(self, input, return_pred_indices)\u001b[0m\n\u001b[1;32m    129\u001b[0m     quant, diff, info \u001b[38;5;241m=\u001b[39m encode_output[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected encode output length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(encode_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Extract index from info if it has the expected structure\u001b[39;00m\n\u001b[1;32m    134\u001b[0m ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected encode output length: 1"
     ]
    }
   ],
   "source": [
    "# Visualize reconstructions\n",
    "vqvae_model = lightning_model.vqvae\n",
    "vqvae_model.eval()\n",
    "device = next(vqvae_model.parameters()).device\n",
    "\n",
    "# Get a batch from validation set\n",
    "batch = next(iter(val_loader))\n",
    "hr_images = vqvae_model.get_input(batch, 'image').to(device)[:8]\n",
    "\n",
    "reconstructed = torch.zeros_like(hr_images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(8):\n",
    "        reconstructed[i], _ = vqvae_model(hr_images[i].unsqueeze(0))\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(hr_images[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vqvae_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding and decoding separately\n",
    "print(\"Testing VQ-VAE encode/decode pipeline...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get one sample\n",
    "    test_img = hr_images[:1]\n",
    "    print(f\"Input shape: {test_img.shape}\")\n",
    "    \n",
    "    # Encode (returns continuous latent)\n",
    "    latent = vqvae_model.encode(test_img)\n",
    "    print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    \n",
    "    # Decode (includes quantization)\n",
    "    reconstructed = vqvae_model.decode(latent)\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    rec_error = torch.nn.functional.l1_loss(reconstructed, test_img)\n",
    "    print(f\"Reconstruction L1 error: {rec_error.item():.6f}\")\n",
    "\n",
    "print(\"\\nVQ-VAE is ready for use in latent diffusion!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-kolmogorov-flow-env",
   "language": "python",
   "name": "diffusion-kolmogorov-flow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
