{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import interpolate\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VQ-VAE training.\n",
    "    Only needs high-resolution images for reconstruction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hr_arrays):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hr_arrays: torch.Tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        assert isinstance(hr_arrays, torch.Tensor), \"hr_arrays must be a torch.Tensor\"\n",
    "        assert hr_arrays.ndim == 4, f\"hr_arrays must be 4D tensor, got shape {hr_arrays.shape}\"\n",
    "        \n",
    "        self.hr = hr_arrays\n",
    "        self.num_samples = hr_arrays.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns dictionary with 'image' key.\n",
    "        VQModelInterface expects shape (H, W, C) for input.\n",
    "        \"\"\"\n",
    "        hr_sample = self.hr[idx]  # Shape: (C, H, W)\n",
    "        hr_sample = hr_sample.permute(1, 2, 0)  # Convert to (H, W, C)\n",
    "        \n",
    "        return {'image': hr_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JAX-CFD normalized data (range [-1,1])\n",
    "path = 'data/data_normalized.h5'\n",
    "hr_data = torch.from_numpy(h5py.File(path, 'r')['hr'][:])\n",
    "\n",
    "# Create RGB tensor from data\n",
    "hr_tensor = torch.stack([hr_data for _ in range(3)], 1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(hr_tensor))\n",
    "val_size = len(hr_tensor) - train_size\n",
    "\n",
    "hr_train = hr_tensor[:train_size]\n",
    "hr_val = hr_tensor[train_size:]\n",
    "\n",
    "train_dataset = VQVAEDataset(hr_train)\n",
    "val_dataset = VQVAEDataset(hr_val)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"HR tensor shape: {hr_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_dataloaders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_vqvae_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualVQVAE(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Wrapper for VQModelInterface that uses manual optimization.\n",
    "    Required for PyTorch Lightning >= 2.0 which doesn't support optimizer_idx.\n",
    "    \"\"\"\n",
    "    def __init__(self, vqvae_model):\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae_model\n",
    "        # Enable manual optimization\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vqvae(x)\n",
    "    \n",
    "    def get_input(self, batch, k):\n",
    "        return self.vqvae.get_input(batch, k)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_ae, opt_disc = self.optimizers()\n",
    "        \n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        # Optimize autoencoder\n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        opt_ae.zero_grad()\n",
    "        self.manual_backward(aeloss)\n",
    "        opt_ae.step()\n",
    "        \n",
    "        # Optimize discriminator\n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"train\"\n",
    "        )\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        self.manual_backward(discloss)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = self.get_input(batch, self.vqvae.image_key)\n",
    "        xrec, qloss, ind = self.vqvae(x, return_pred_indices=True)\n",
    "        \n",
    "        aeloss, log_dict_ae = self.vqvae.loss(\n",
    "            qloss, x, xrec, 0, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        discloss, log_dict_disc = self.vqvae.loss(\n",
    "            qloss, x, xrec, 1, self.global_step,\n",
    "            last_layer=self.vqvae.get_last_layer(),\n",
    "            split=\"val\",\n",
    "            predicted_indices=ind\n",
    "        )\n",
    "        \n",
    "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"], prog_bar=True, sync_dist=True)\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.vqvae.learning_rate\n",
    "        \n",
    "        opt_ae = torch.optim.Adam(\n",
    "            list(self.vqvae.encoder.parameters()) +\n",
    "            list(self.vqvae.decoder.parameters()) +\n",
    "            list(self.vqvae.quantize.parameters()) +\n",
    "            list(self.vqvae.quant_conv.parameters()) +\n",
    "            list(self.vqvae.post_quant_conv.parameters()),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        opt_disc = torch.optim.Adam(\n",
    "            self.vqvae.loss.discriminator.parameters(),\n",
    "            lr=lr, betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        return [opt_ae, opt_disc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_vqvae_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VQ-VAE config\n",
    "# Based on the VQModelInterface structure from autoencoder.py\n",
    "vqvae_config = OmegaConf.create({\n",
    "    'target': 'ldm.autoencoder.VQModelInterface',\n",
    "    'params': {\n",
    "        'embed_dim': 3,\n",
    "        'n_embed': 8192,\n",
    "        'ddconfig': {\n",
    "            'double_z': False,\n",
    "            'z_channels': 3,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 128,\n",
    "            'ch_mult': [1, 2, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'lossconfig': {\n",
    "            'target': 'ldm.vqperceptual.VQLPIPSWithDiscriminator',\n",
    "            'params': {\n",
    "                'disc_conditional': False,\n",
    "                'disc_in_channels': 3,\n",
    "                'disc_start': 10000,\n",
    "                'disc_weight': 0.8,\n",
    "                'codebook_weight': 1.0\n",
    "            }\n",
    "        },\n",
    "        'image_key': 'image',\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"VQ-VAE config created\")\n",
    "print(OmegaConf.to_yaml(vqvae_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate VQModelInterface from config\n",
    "vqvae_model = instantiate_from_config(vqvae_config)\n",
    "\n",
    "# Set learning rate (VQModel expects this attribute)\n",
    "vqvae_model.learning_rate = 4.5e-6\n",
    "\n",
    "# Optional: Load pretrained weights\n",
    "load_pretrained = False\n",
    "if load_pretrained:\n",
    "    ckpt_path = \"model.ckpt\"\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    \n",
    "    # Extract only VQ-VAE weights (first_stage_model)\n",
    "    vqvae_sd = {k.replace('first_stage_model.', ''): v \n",
    "                for k, v in sd.items() \n",
    "                if k.startswith('first_stage_model.')}\n",
    "    \n",
    "    vqvae_model.load_state_dict(vqvae_sd, strict=False)\n",
    "    print(\"Loaded pretrained VQ-VAE weights\")\n",
    "else:\n",
    "    print(\"Training VQ-VAE from scratch\")\n",
    "\n",
    "print(f\"\\nVQ-VAE model type: {type(vqvae_model)}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vqvae_model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in vqvae_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/vqvae',\n",
    "    filename='vqvae-{epoch:02d}-{val/rec_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val/rec_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Setup trainer\n",
    "# Note: VQModel uses 2 optimizers (autoencoder + discriminator)\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    logger=True,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Max epochs: {trainer.max_epochs}\")\n",
    "print(f\"Gradient clip: {trainer.gradient_clip_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VQ-VAE using VQModelInterface\n",
    "print(\"Starting VQ-VAE training...\")\n",
    "print(\"Note: VQModel uses 2 optimizers (autoencoder + discriminator)\")\n",
    "trainer.fit(vqvae_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final VQ-VAE model\n",
    "vqvae_path = \"vqvae_trained.ckpt\"\n",
    "torch.save({\n",
    "    'state_dict': vqvae_model.state_dict(),\n",
    "    'config': vqvae_config,\n",
    "}, vqvae_path)\n",
    "\n",
    "print(f\"VQ-VAE saved to {vqvae_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_reconstructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "vqvae_model.eval()\n",
    "device = next(vqvae_model.parameters()).device\n",
    "\n",
    "# Get a batch from validation set\n",
    "batch = next(iter(val_loader))\n",
    "hr_images = vqvae_model.get_input(batch, 'image').to(device)[:8]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, _ = vqvae_model(hr_images)\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(hr_images[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vqvae_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding and decoding separately\n",
    "print(\"Testing VQ-VAE encode/decode pipeline...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get one sample\n",
    "    test_img = hr_images[:1]\n",
    "    print(f\"Input shape: {test_img.shape}\")\n",
    "    \n",
    "    # Encode (returns continuous latent)\n",
    "    latent = vqvae_model.encode(test_img)\n",
    "    print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    \n",
    "    # Decode (includes quantization)\n",
    "    reconstructed = vqvae_model.decode(latent)\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    rec_error = torch.nn.functional.l1_loss(reconstructed, test_img)\n",
    "    print(f\"Reconstruction L1 error: {rec_error.item():.6f}\")\n",
    "\n",
    "print(\"\\nVQ-VAE is ready for use in latent diffusion!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-kolmogorov-flow-env",
   "language": "python",
   "name": "diffusion-kolmogorov-flow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
