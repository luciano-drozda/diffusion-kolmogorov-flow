{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933a525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/coop/drozda/torch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 113.62 M params.\n",
      "Keeping EMAs of 308.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "LatentDiffusion(\n",
      "  (model): DiffusionWrapper(\n",
      "    (diffusion_model): UNetModel(\n",
      "      (time_embed): Sequential(\n",
      "        (0): Linear(in_features=160, out_features=640, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=640, out_features=640, bias=True)\n",
      "      )\n",
      "      (input_blocks): ModuleList(\n",
      "        (0): TimestepEmbedSequential(\n",
      "          (0): Conv2d(6, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=160, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Identity()\n",
      "          )\n",
      "        )\n",
      "        (3): TimestepEmbedSequential(\n",
      "          (0): Downsample(\n",
      "            (op): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (4): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (5): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Identity()\n",
      "          )\n",
      "        )\n",
      "        (6): TimestepEmbedSequential(\n",
      "          (0): Downsample(\n",
      "            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (7-8): 2 x TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Identity()\n",
      "          )\n",
      "        )\n",
      "        (9): TimestepEmbedSequential(\n",
      "          (0): Downsample(\n",
      "            (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (10): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): AttentionBlock(\n",
      "            (norm): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (qkv): Conv1d(640, 1920, kernel_size=(1,), stride=(1,))\n",
      "            (attention): QKVAttentionLegacy()\n",
      "            (proj_out): Conv1d(640, 640, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "        (11): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Identity()\n",
      "          )\n",
      "          (1): AttentionBlock(\n",
      "            (norm): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (qkv): Conv1d(640, 1920, kernel_size=(1,), stride=(1,))\n",
      "            (attention): QKVAttentionLegacy()\n",
      "            (proj_out): Conv1d(640, 640, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (middle_block): TimestepEmbedSequential(\n",
      "        (0): ResBlock(\n",
      "          (in_layers): Sequential(\n",
      "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (h_upd): Identity()\n",
      "          (x_upd): Identity()\n",
      "          (emb_layers): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "          )\n",
      "          (out_layers): Sequential(\n",
      "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0, inplace=False)\n",
      "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (skip_connection): Identity()\n",
      "        )\n",
      "        (1): AttentionBlock(\n",
      "          (norm): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "          (qkv): Conv1d(640, 1920, kernel_size=(1,), stride=(1,))\n",
      "          (attention): QKVAttentionLegacy()\n",
      "          (proj_out): Conv1d(640, 640, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ResBlock(\n",
      "          (in_layers): Sequential(\n",
      "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (h_upd): Identity()\n",
      "          (x_upd): Identity()\n",
      "          (emb_layers): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "          )\n",
      "          (out_layers): Sequential(\n",
      "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0, inplace=False)\n",
      "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (skip_connection): Identity()\n",
      "        )\n",
      "      )\n",
      "      (output_blocks): ModuleList(\n",
      "        (0-1): 2 x TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): AttentionBlock(\n",
      "            (norm): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (qkv): Conv1d(640, 1920, kernel_size=(1,), stride=(1,))\n",
      "            (attention): QKVAttentionLegacy()\n",
      "            (proj_out): Conv1d(640, 640, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "        (2): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=640, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): AttentionBlock(\n",
      "            (norm): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "            (qkv): Conv1d(640, 1920, kernel_size=(1,), stride=(1,))\n",
      "            (attention): QKVAttentionLegacy()\n",
      "            (proj_out): Conv1d(640, 640, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (2): Upsample(\n",
      "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (3): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (4): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (5): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Upsample(\n",
      "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (6-7): 2 x TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (8): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 480, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(480, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=320, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(480, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Upsample(\n",
      "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (9): TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 480, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(480, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=160, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (10-11): 2 x TimestepEmbedSequential(\n",
      "          (0): ResBlock(\n",
      "            (in_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (h_upd): Identity()\n",
      "            (x_upd): Identity()\n",
      "            (emb_layers): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=640, out_features=160, bias=True)\n",
      "            )\n",
      "            (out_layers): Sequential(\n",
      "              (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "            (skip_connection): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): GroupNorm32(32, 160, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(160, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model_ema): LitEma()\n",
      "  (first_stage_model): VQModelInterface(\n",
      "    (encoder): Encoder(\n",
      "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (down): ModuleList(\n",
      "        (0): Module(\n",
      "          (block): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "          (downsample): Downsample(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (block): ModuleList(\n",
      "            (0): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "          (downsample): Downsample(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "        (2): Module(\n",
      "          (block): ModuleList(\n",
      "            (0): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "        )\n",
      "      )\n",
      "      (mid): Module(\n",
      "        (block_1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (attn_1): AttnBlock(\n",
      "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (block_2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv_out): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mid): Module(\n",
      "        (block_1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (attn_1): AttnBlock(\n",
      "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (block_2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (up): ModuleList(\n",
      "        (0): Module(\n",
      "          (block): ModuleList(\n",
      "            (0): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1-2): 2 x ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (block): ModuleList(\n",
      "            (0): ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1-2): 2 x ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "          (upsample): Upsample(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (2): Module(\n",
      "          (block): ModuleList(\n",
      "            (0-2): 3 x ResnetBlock(\n",
      "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (attn): ModuleList()\n",
      "          (upsample): Upsample(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (loss): Identity()\n",
      "    (quantize): VectorQuantizer2(\n",
      "      (embedding): Embedding(8192, 3)\n",
      "    )\n",
      "    (quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (cond_stage_model): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy                as np\n",
    "import matplotlib.pyplot    as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch\n",
    "import h5py\n",
    "import yaml\n",
    "from   tqdm      import tqdm\n",
    "from   omegaconf import OmegaConf\n",
    "\n",
    "from   ldm.util  import instantiate_from_config\n",
    "\n",
    "# Instantiate model from config YAML file\n",
    "config_path = \"ldm/yaml_config.yaml\"\n",
    "config      = OmegaConf.load(config_path)\n",
    "model       = instantiate_from_config(config.model)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = \"model_tuned.ckpt\"\n",
    "sd        = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd, strict=False)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9e18fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model        = model.to(device)\n",
    "model.logvar = model.logvar.to(model.device)\n",
    "print(f\"model.device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f397c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JAX-CFD normalized data (range [-1,1])\n",
    "# See 'data/generate.ipynb'\n",
    "path       = 'data/data_normalized.h5'\n",
    "hr_data    = torch.from_numpy(h5py.File(path, 'r')['hr'][:])\n",
    "lr_data    = torch.from_numpy(h5py.File(path, 'r')['lr'][:])\n",
    "hr_tensor  = torch.stack([hr_data for _ in range(3)], 1)        # create RGB tensor from data\n",
    "lr_tensor  = torch.stack([lr_data for _ in range(3)], 1)        # create RGB tensor from data\n",
    "# img_idx    = 0\n",
    "hr_image   = hr_tensor                 \n",
    "lr_image   = lr_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a46b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([512, 3, 256, 256]) torch.float32\n",
      "cuda:0 torch.Size([512, 3, 64, 64]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(hr_image.device, hr_image.shape, hr_image.dtype)\n",
    "print(lr_image.device, lr_image.shape, lr_image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28d4f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 3, 256, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-allocate tensor which will store LDM output\n",
    "hr_ldm = torch.zeros_like(hr_image).to(device)\n",
    "hr_ldm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413a6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1   6  11  16  21  26  31  36  41  46  51  56  61  66  71  76  81  86\n",
      "  91  96 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171 176\n",
      " 181 186 191 196 201 206 211 216 221 226 231 236 241 246 251 256 261 266\n",
      " 271 276 281 286 291 296 301 306 311 316 321 326 331 336 341 346 351 356\n",
      " 361 366 371 376 381 386 391 396 401 406 411 416 421 426 431 436 441 446\n",
      " 451 456 461 466 471 476 481 486 491 496 501 506 511 516 521 526 531 536\n",
      " 541 546 551 556 561 566 571 576 581 586 591 596 601 606 611 616 621 626\n",
      " 631 636 641 646 651 656 661 666 671 676 681 686 691 696 701 706 711 716\n",
      " 721 726 731 736 741 746 751 756 761 766 771 776 781 786 791 796 801 806\n",
      " 811 816 821 826 831 836 841 846 851 856 861 866 871 876 881 886 891 896\n",
      " 901 906 911 916 921 926 931 936 941 946 951 956 961 966 971 976 981 986\n",
      " 991 996]\n",
      "Selected alphas for ddim sampler: a_t: tensor([9.9700e-01, 9.8941e-01, 9.8171e-01, 9.7391e-01, 9.6600e-01, 9.5799e-01,\n",
      "        9.4988e-01, 9.4167e-01, 9.3337e-01, 9.2496e-01, 9.1647e-01, 9.0788e-01,\n",
      "        8.9921e-01, 8.9045e-01, 8.8161e-01, 8.7269e-01, 8.6368e-01, 8.5461e-01,\n",
      "        8.4546e-01, 8.3624e-01, 8.2696e-01, 8.1761e-01, 8.0820e-01, 7.9873e-01,\n",
      "        7.8921e-01, 7.7963e-01, 7.7001e-01, 7.6034e-01, 7.5064e-01, 7.4089e-01,\n",
      "        7.3111e-01, 7.2130e-01, 7.1145e-01, 7.0159e-01, 6.9170e-01, 6.8180e-01,\n",
      "        6.7188e-01, 6.6195e-01, 6.5201e-01, 6.4207e-01, 6.3213e-01, 6.2220e-01,\n",
      "        6.1227e-01, 6.0235e-01, 5.9245e-01, 5.8257e-01, 5.7270e-01, 5.6286e-01,\n",
      "        5.5305e-01, 5.4327e-01, 5.3353e-01, 5.2383e-01, 5.1416e-01, 5.0454e-01,\n",
      "        4.9498e-01, 4.8546e-01, 4.7600e-01, 4.6659e-01, 4.5725e-01, 4.4797e-01,\n",
      "        4.3875e-01, 4.2961e-01, 4.2054e-01, 4.1155e-01, 4.0263e-01, 3.9379e-01,\n",
      "        3.8504e-01, 3.7637e-01, 3.6779e-01, 3.5930e-01, 3.5090e-01, 3.4260e-01,\n",
      "        3.3439e-01, 3.2628e-01, 3.1828e-01, 3.1037e-01, 3.0257e-01, 2.9487e-01,\n",
      "        2.8728e-01, 2.7980e-01, 2.7242e-01, 2.6516e-01, 2.5801e-01, 2.5097e-01,\n",
      "        2.4405e-01, 2.3723e-01, 2.3054e-01, 2.2396e-01, 2.1750e-01, 2.1115e-01,\n",
      "        2.0492e-01, 1.9881e-01, 1.9281e-01, 1.8693e-01, 1.8117e-01, 1.7553e-01,\n",
      "        1.7000e-01, 1.6459e-01, 1.5930e-01, 1.5413e-01, 1.4907e-01, 1.4412e-01,\n",
      "        1.3929e-01, 1.3458e-01, 1.2997e-01, 1.2548e-01, 1.2110e-01, 1.1683e-01,\n",
      "        1.1267e-01, 1.0862e-01, 1.0467e-01, 1.0083e-01, 9.7098e-02, 9.3466e-02,\n",
      "        8.9936e-02, 8.6506e-02, 8.3176e-02, 7.9943e-02, 7.6806e-02, 7.3763e-02,\n",
      "        7.0814e-02, 6.7956e-02, 6.5187e-02, 6.2507e-02, 5.9913e-02, 5.7404e-02,\n",
      "        5.4978e-02, 5.2633e-02, 5.0368e-02, 4.8181e-02, 4.6070e-02, 4.4033e-02,\n",
      "        4.2069e-02, 4.0176e-02, 3.8352e-02, 3.6596e-02, 3.4906e-02, 3.3279e-02,\n",
      "        3.1715e-02, 3.0212e-02, 2.8768e-02, 2.7380e-02, 2.6049e-02, 2.4772e-02,\n",
      "        2.3547e-02, 2.2372e-02, 2.1247e-02, 2.0170e-02, 1.9139e-02, 1.8153e-02,\n",
      "        1.7209e-02, 1.6308e-02, 1.5446e-02, 1.4624e-02, 1.3839e-02, 1.3090e-02,\n",
      "        1.2376e-02, 1.1696e-02, 1.1048e-02, 1.0431e-02, 9.8438e-03, 9.2854e-03,\n",
      "        8.7545e-03, 8.2502e-03, 7.7712e-03, 7.3166e-03, 6.8852e-03, 6.4762e-03,\n",
      "        6.0886e-03, 5.7214e-03, 5.3738e-03, 5.0448e-03, 4.7336e-03, 4.4395e-03,\n",
      "        4.1616e-03, 3.8992e-03, 3.6515e-03, 3.4178e-03, 3.1975e-03, 2.9899e-03,\n",
      "        2.7943e-03, 2.6103e-03, 2.4371e-03, 2.2743e-03, 2.1212e-03, 1.9774e-03,\n",
      "        1.8425e-03, 1.7158e-03, 1.5971e-03, 1.4857e-03, 1.3815e-03, 1.2838e-03,\n",
      "        1.1925e-03, 1.1070e-03, 1.0271e-03, 9.5252e-04, 8.8285e-04, 8.1784e-04,\n",
      "        7.5722e-04, 7.0070e-04]); a_(t-1): [9.98499990e-01 9.96995628e-01 9.89408076e-01 9.81712461e-01\n",
      " 9.73910332e-01 9.66003478e-01 9.57993686e-01 9.49882805e-01\n",
      " 9.41672802e-01 9.33365643e-01 9.24963593e-01 9.16468799e-01\n",
      " 9.07883465e-01 8.99209976e-01 8.90450835e-01 8.81608486e-01\n",
      " 8.72685492e-01 8.63684475e-01 8.54608178e-01 8.45459402e-01\n",
      " 8.36240888e-01 8.26955676e-01 8.17606628e-01 8.08196783e-01\n",
      " 7.98729181e-01 7.89207041e-01 7.79633462e-01 7.70011604e-01\n",
      " 7.60344863e-01 7.50636518e-01 7.40889788e-01 7.31108189e-01\n",
      " 7.21295118e-01 7.11453974e-01 7.01588213e-01 6.91701353e-01\n",
      " 6.81796908e-01 6.71878457e-01 6.61949456e-01 6.52013421e-01\n",
      " 6.42074049e-01 6.32134855e-01 6.22199297e-01 6.12271070e-01\n",
      " 6.02353632e-01 5.92450619e-01 5.82565486e-01 5.72701752e-01\n",
      " 5.62862873e-01 5.53052425e-01 5.43273747e-01 5.33530295e-01\n",
      " 5.23825407e-01 5.14162481e-01 5.04544795e-01 4.94975537e-01\n",
      " 4.85457987e-01 4.75995272e-01 4.66590524e-01 4.57246780e-01\n",
      " 4.47966993e-01 4.38754171e-01 4.29611117e-01 4.20540661e-01\n",
      " 4.11545515e-01 4.02628362e-01 3.93791795e-01 3.85038286e-01\n",
      " 3.76370281e-01 3.67790163e-01 3.59300196e-01 3.50902528e-01\n",
      " 3.42599303e-01 3.34392518e-01 3.26284140e-01 3.18275958e-01\n",
      " 3.10369730e-01 3.02567154e-01 2.94869751e-01 2.87279010e-01\n",
      " 2.79796332e-01 2.72422969e-01 2.65160143e-01 2.58008927e-01\n",
      " 2.50970334e-01 2.44045272e-01 2.37234563e-01 2.30538905e-01\n",
      " 2.23958954e-01 2.17495233e-01 2.11148158e-01 2.04918116e-01\n",
      " 1.98805347e-01 1.92810029e-01 1.86932221e-01 1.81171954e-01\n",
      " 1.75529107e-01 1.70003504e-01 1.64594904e-01 1.59302965e-01\n",
      " 1.54127255e-01 1.49067298e-01 1.44122496e-01 1.39292210e-01\n",
      " 1.34575740e-01 1.29972294e-01 1.25481009e-01 1.21100977e-01\n",
      " 1.16831213e-01 1.12670675e-01 1.08618267e-01 1.04672834e-01\n",
      " 1.00833170e-01 9.70980227e-02 9.34660733e-02 8.99359882e-02\n",
      " 8.65063667e-02 8.31757709e-02 7.99427405e-02 7.68057629e-02\n",
      " 7.37633035e-02 7.08137900e-02 6.79556355e-02 6.51872158e-02\n",
      " 6.25068843e-02 5.99129871e-02 5.74038364e-02 5.49777411e-02\n",
      " 5.26329949e-02 5.03678769e-02 4.81806658e-02 4.60696258e-02\n",
      " 4.40330207e-02 4.20691147e-02 4.01761681e-02 3.83524522e-02\n",
      " 3.65962349e-02 3.49057876e-02 3.32794003e-02 3.17153633e-02\n",
      " 3.02119832e-02 2.87675764e-02 2.73804776e-02 2.60490347e-02\n",
      " 2.47716121e-02 2.35465951e-02 2.23723892e-02 2.12474186e-02\n",
      " 2.01701298e-02 1.91389974e-02 1.81525107e-02 1.72091946e-02\n",
      " 1.63075887e-02 1.54462699e-02 1.46238338e-02 1.38389086e-02\n",
      " 1.30901476e-02 1.23762358e-02 1.16958842e-02 1.10478355e-02\n",
      " 1.04308613e-02 9.84376203e-03 9.28536989e-03 8.75454582e-03\n",
      " 8.25018249e-03 7.77120190e-03 7.31655490e-03 6.88522449e-03\n",
      " 6.47622254e-03 6.08859072e-03 5.72140003e-03 5.37375081e-03\n",
      " 5.04477276e-03 4.73362394e-03 4.43948992e-03 4.16158466e-03\n",
      " 3.89915030e-03 3.65145458e-03 3.41779250e-03 3.19748512e-03\n",
      " 2.98987888e-03 2.79434491e-03 2.61027925e-03 2.43710168e-03\n",
      " 2.27425504e-03 2.12120567e-03 1.97744113e-03 1.84247119e-03\n",
      " 1.71582669e-03 1.59705838e-03 1.48573739e-03 1.38145371e-03\n",
      " 1.28381618e-03 1.19245134e-03 1.10700354e-03 1.02713390e-03\n",
      " 9.52519709e-04 8.82853987e-04 8.17844877e-04 7.57215254e-04]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Data shape for DDIM sampling is (512, 3, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [02:44<00:00,  1.22it/s]\n",
      "  2%|▏         | 12/512 [00:00<00:09, 53.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent samples shape:      torch.Size([1, 3, 64, 64])\n",
      "High-res LDM output shape: torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Call trained model on JAX-CFD low-res data ('lr_image' as input / conditioning)\n",
    "\n",
    "nb_snapshots = lr_image.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # For sampling, you need:\n",
    "    # - conditioning (low-res image)\n",
    "    # - batch size\n",
    "    # - return intermediates (optional)\n",
    "\n",
    "    # Method 2: With DDIM sampling (common for inference)\n",
    "    from ldm.ddim import DDIMSampler\n",
    "\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "    # Sample shape: [C, H, W] in latent space\n",
    "    shape = (3, 64, 64)\n",
    "\n",
    "    samples_ddim, _ = sampler.sample(\n",
    "        200,           # number of steps\n",
    "        nb_snapshots,  # batch_size\n",
    "        shape,\n",
    "        conditioning=lr_image\n",
    "    )\n",
    "    \n",
    "    # Decode from latent to pixel space using autoencoder\n",
    "    for i_snap in tqdm(range(nb_snapshots)):\n",
    "        sample_ddim    = samples_ddim[i_snap].unsqueeze(0)\n",
    "        hr_ldm[i_snap] = model.decode_first_stage(sample_ddim)\n",
    "        if i_snap == 0:\n",
    "            print(f\"Latent samples shape:      {sample_ddim.shape}\")    # Should be [1, 3,  64,  64]\n",
    "            print(f\"High-res LDM output shape: {hr_ldm[i_snap].shape}\") # Should be    [3, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaef732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([512, 3, 256, 256]) torch.float32\n",
      "cpu torch.Size([512, 3, 256, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "hr_ldm = hr_ldm.to(\"cpu\")\n",
    "print(hr_image.device, hr_image.shape, hr_image.dtype) # REFERENCE DATA\n",
    "print(hr_ldm.device,   hr_ldm.shape,   hr_ldm.dtype)   # LDM OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96b4c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(-1.) tensor(0.1323)\n",
      "tensor(1.0952) tensor(-0.9365) tensor(0.1402)\n",
      "(vmax, vmin): (1.0951557159423828, -1.0)\n"
     ]
    }
   ],
   "source": [
    "print(hr_image.max(), hr_image.min(), torch.mean(hr_image)) # REFERENCE DATA\n",
    "print(hr_ldm.max(),   hr_ldm.min(),   torch.mean(hr_ldm))   # LDM OUTPUT\n",
    "vmax = max(hr_ldm.max().item(), hr_image.max().item())\n",
    "vmin = min(hr_ldm.min().item(), hr_image.min().item())\n",
    "print(f\"(vmax, vmin): ({vmax}, {vmin})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "176f3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDM output to HDF5 file\n",
    "with h5py.File('data/data_normalized_ldm.h5', 'w') as f:\n",
    "    f.create_dataset('data', data=hr_ldm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4f246282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3912520/166133717.py:47: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout(rect=[0, 0.01, 1, 0.99])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ldm.gif\n"
     ]
    }
   ],
   "source": [
    "# Save .gif comparing ground-truth and LDM output for each snapshot\n",
    "# It also shows LDM input / conditioning\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "fig, axes   = plt.subplots(1, 3, figsize=(16, 7))\n",
    "colormap    = 'RdBu'\n",
    "images      = []\n",
    "series_list = [hr_image,       hr_ldm,       lr_image.to('cpu')]\n",
    "titles      = ['Ground-truth\\n$(256^2)$', 'LDM Output\\n$(256^2)$', 'LDM Input / Conditioning\\n$(64^2)$']\n",
    "\n",
    "# Initialize images\n",
    "i_snap    = 0\n",
    "i_channel = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    matrix = series_list[i][i_snap,i_channel]\n",
    "    im     = ax.imshow(matrix, cmap=colormap, vmin=vmin, vmax=vmax, aspect='equal')\n",
    "    images.append(im)\n",
    "    ax.set_title(titles[i], fontsize=26)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "def update(i_snap):\n",
    "    for i, im in enumerate(images):\n",
    "        matrix = series_list[i][i_snap,i_channel]\n",
    "        im.set_data(matrix)\n",
    "\n",
    "    return []\n",
    "\n",
    "# Create ScalarMappable for the colorbar\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = ScalarMappable(norm=norm, cmap=colormap)\n",
    "sm.set_array([])\n",
    "\n",
    "# Add colorbar at the bottom, spanning all subplots\n",
    "# Adjust layout to make room for the colorbar at the bottom\n",
    "fig.subplots_adjust(bottom=0.05)  # Leave space at the bottom\n",
    "\n",
    "# Create axes for colorbar at the bottom\n",
    "cbar_ax = fig.add_axes([0.15, 0.06, 0.7, 0.03])  # [left, bottom, width, height]\n",
    "\n",
    "# Create horizontal colorbar\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "# Apply tight layout (adjust rect to leave space for colorbar)\n",
    "fig.tight_layout(rect=[0, 0.01, 1, 0.99])\n",
    "\n",
    "# Create animation\n",
    "fps  = 5\n",
    "anim = animation.FuncAnimation(fig, update, frames=min(25, hr_image.shape[0]), \n",
    "                               interval=1000//fps, blit=False)\n",
    "\n",
    "# Save animation\n",
    "anim_path = 'ldm.gif'\n",
    "anim.save(anim_path, writer='pillow', fps=fps)\n",
    "plt.close(fig)\n",
    "print(f\"GIF saved to {anim_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f4f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-kolmogorov-flow-env",
   "language": "python",
   "name": "diffusion-kolmogorov-flow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
